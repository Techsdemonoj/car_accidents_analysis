{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efee5169-8d12-4017-9c71-9fab1501ccb9",
   "metadata": {},
   "source": [
    "# üöó Traffic Accident Analysis: Complex Questions  \n",
    "\n",
    "## 1Ô∏è‚É£ Accident Severity and Conditions  \n",
    "- How do **weather conditions, light conditions, and road surface conditions** impact **accident severity**?  \n",
    "\n",
    "## 2Ô∏è‚É£ Temporal Accident Patterns  \n",
    "- What are the **most accident-prone months, days, and time slots**?  \n",
    "- Is there a pattern in **weekdays vs. weekends**?  \n",
    "\n",
    "## 3Ô∏è‚É£ Geospatial Accident Hotspots  \n",
    "- Using **latitude and longitude**, can we identify accident hotspots using **clustering algorithms** (e.g., DBSCAN, K-Means)?  \n",
    "\n",
    "## 4Ô∏è‚É£ Speed Limit vs. Severity  \n",
    "- What is the **relationship between speed limits and accident severity**?  \n",
    "- Are higher speed limits linked to **more severe accidents**?  \n",
    "\n",
    "## 5Ô∏è‚É£ Vehicle Type and Casualties  \n",
    "- Which **vehicle types** are involved in the most severe accidents?  \n",
    "- Do certain vehicles correlate with **higher casualty numbers**?  \n",
    "\n",
    "## 6Ô∏è‚É£ Junction Control and Accidents  \n",
    "- Does the **presence or absence of junction control** (e.g., traffic signals, roundabouts) impact accident frequency and severity?  \n",
    "\n",
    "## 7Ô∏è‚É£ Multi-Vehicle Collisions  \n",
    "- What percentage of accidents involve **multiple vehicles**?  \n",
    "- How does **severity** change as the number of vehicles increases?  \n",
    "\n",
    "## 8Ô∏è‚É£ Rural vs. Urban Accident Trends  \n",
    "- Are **urban areas more accident-prone** than rural areas?  \n",
    "- How does accident severity differ between these regions?  \n",
    "\n",
    "## 9Ô∏è‚É£ District-Level Accident Trends  \n",
    "- Which **local authority districts** report the highest number of accidents?  \n",
    "- Are certain regions more accident-prone due to **weather, traffic, or road types**?  \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3b6bc-5da6-465b-a719-3bd5cde3bf6e",
   "metadata": {},
   "source": [
    "### Installing PySpark\n",
    "\n",
    "| Method            | Ease of Use | Dependencies Managed | Best For                                | Notes |\n",
    "|------------------|------------|----------------------|----------------------------------------|-------|\n",
    "| Using pip       | High       | Partial (requires Java) | Quick local setups, clusters | Experimental, specify Hadoop version if needed, Python 3.8+ required |\n",
    "| Using conda     | High       | Yes (includes Java)  | Anaconda users, data science | Community-maintained, may lag behind releases |\n",
    "| Manual Download | Low        | No (manual setup)    | Full control, custom setups | Requires setting environment variables, more complex |\n",
    "| From Source     | Very Low   | No (manual setup)    | Developers contributing to Spark | Involves building, not for general users |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a6302-2a80-4bbd-aadf-43d7bef6d38d",
   "metadata": {},
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b5e49-79d0-4a37-a282-36fcff6d4ddc",
   "metadata": {},
   "source": [
    "### importing findspark\n",
    "* What it does: The findspark.init() function initializes the PySpark environment by adding PySpark to sys.path at runtime, making it importable. According to the GitHub repository findspark GitHub Repository, it uses the SPARK_HOME environment variable by default to locate the Spark installation. If SPARK_HOME is not set, it checks other possible locations, such as /usr/local/opt/apache-spark/libexec for OS X with brew install apache-spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb311bf8-bf91-446c-a6b8-84ee870aa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7807b-5236-437c-ab0f-86ac94cf4a31",
   "metadata": {},
   "source": [
    "### Checking PySpark Version\n",
    "\n",
    "You can check the installed PySpark version using the following code:\n",
    "\n",
    "```python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d1682d-48d7-4757-b0d4-eab986bf7c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CheckVersion\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f390d821-93f3-4420-a5e6-187469c88471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb53a3e-1527-4b21-b921-2cf8b48ae375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CheckVersion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c38025d400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51c514-3545-4a99-91be-e81b333531d8",
   "metadata": {},
   "source": [
    "## Read and Check and Selecting method \n",
    "* PySpark Dataframe\n",
    "* Reading the datasets\n",
    "* Checking the datatypes of the columns(schema)\n",
    "* Selecting Columns and Indexing\n",
    "* Check Describe option similer to Pandas\n",
    "* Adding columns\n",
    "* Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5d3c9-8c70-4a86-9d4b-7a88f40d2519",
   "metadata": {},
   "source": [
    "### Reading a CSV File in PySpark\n",
    "\n",
    "* You can read a CSV file using PySpark with the following code:\n",
    "\n",
    "* SparkSession.builder.appName(\"ReadCSV\").getOrCreate() initializes a Spark session.\n",
    "* spark.read.csv(\"file location with file type\", header=True, inferSchema=True) reads the CSV file:\n",
    "* header=True: Treats the first row as column headers.\n",
    "* inferSchema=True: Automatically detects column data types.\n",
    "* df_csv.show() displays the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1353370f-1725-4828-bbb2-13e4e32fe1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, dayofweek, month, hour,when\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"data\").getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4c30f754-e740-4bcb-9405-9480c21d5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+\n",
      "|Accident_Index|Accident Date|Month|Year|Day_of_Week|    Junction_Control|     Junction_Detail|Accident_Severity| Latitude|Light_Conditions|Local_Authority_(District)|Carriageway_Hazards|Longitude|Number_of_Casualties|Number_of_Vehicles|       Police_Force|Road_Surface_Conditions|         Road_Type|Speed_limit|               Time|Urban_or_Rural_Area|Weather_Conditions|        Vehicle_Type|\n",
      "+--------------+-------------+-----+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+\n",
      "| 200901BS70001|   01-01-2021|  Jan|2021|   Thursday|Give way or uncon...|T or staggered ju...|          Serious|51.512273|        Daylight|      Kensington and Ch...|               None|-0.201349|                   1|                 2|Metropolitan Police|                    Dry|    One way street|         30|2025-03-20 15:11:00|              Urban|Fine no high winds|                 Car|\n",
      "| 200901BS70002|   05-01-2021|  Jan|2021|     Monday|Give way or uncon...|          Crossroads|          Serious|51.514399|        Daylight|      Kensington and Ch...|               None|-0.199248|                  11|                 2|Metropolitan Police|            Wet or damp|Single carriageway|         30|2025-03-20 10:59:00|              Urban|Fine no high winds|Taxi/Private hire...|\n",
      "+--------------+-------------+-----+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file\n",
    "df_data = spark.read.csv(r\"D:\\Resume_project\\pyspark_analysis\\AccidentData.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows\n",
    "df_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a7fbd2-839c-4d75-bd48-240b17734a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CheckVersion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c38025d400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d9a27-cb66-4695-9cf0-acbe3cc15fe7",
   "metadata": {},
   "source": [
    "### Reading JSON and Parquet Files in PySpark  \n",
    "\n",
    "#### Reading a JSON File  \n",
    "- **`spark.read.json(\"file.json\")`** is used to read JSON files.  \n",
    "- **Schema Inference**: PySpark automatically detects the schema, but specifying a schema is recommended for large files.  \n",
    "- **Nested Data**: JSON supports hierarchical data, which can be accessed using dot notation.  \n",
    "\n",
    "#### Reading a Parquet File  \n",
    "- **`spark.read.parquet(\"file.parquet\")`** reads Parquet files efficiently.  \n",
    "- **Optimized for Performance**: Parquet is a columnar storage format, making queries faster and reducing storage space.  \n",
    "- **Schema Preservation**: Unlike CSV, Parquet maintains schema and data types.  \n",
    "- **Partitioning**: Supports partitioning for faster query execution.  \n",
    "\n",
    "üöÄ **Tip**: Use Parquet for large datasets due to its compression and query efficiency.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762ce237-ce35-4d44-8463-65a0364cda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"drivers.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0d5e26f4-10ae-438f-a1ce-1e1e3036417f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+---------+-----------------+-----------+------+--------------------+\n",
      "|code|       dob|driverId|driverRef|             name|nationality|number|                 url|\n",
      "+----+----------+--------+---------+-----------------+-----------+------+--------------------+\n",
      "| HAM|1985-01-07|       1| hamilton|{Lewis, Hamilton}|    British|    44|http://en.wikiped...|\n",
      "| HEI|1977-05-10|       2| heidfeld| {Nick, Heidfeld}|     German|    \\N|http://en.wikiped...|\n",
      "+----+----------+--------+---------+-----------------+-----------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b193c4f-0cdf-45be-8458-5c473ea76db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[code: string, dob: string, driverId: bigint, driverRef: string, name: struct<forename:string,surname:string>, nationality: string, number: string, url: string]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b69eb3-0c26-4880-848b-93d204f5b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Accident_Index: string, Accident Date: string, Month: string, Year: string, Day_of_Week: string, Junction_Control: string, Junction_Detail: string, Accident_Severity: string, Latitude: string, Light_Conditions: string, Local_Authority_(District): string, Carriageway_Hazards: string, Longitude: string, Number_of_Casualties: string, Number_of_Vehicles: string, Police_Force: string, Road_Surface_Conditions: string, Road_Type: string, Speed_limit: string, Urban_or_Rural_Area: string, Weather_Conditions: string, Vehicle_Type: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da28402-329a-4b4d-a6bc-8249e1d1ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accident_Index: string (nullable = true)\n",
      " |-- Accident Date: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Day_of_Week: string (nullable = true)\n",
      " |-- Junction_Control: string (nullable = true)\n",
      " |-- Junction_Detail: string (nullable = true)\n",
      " |-- Accident_Severity: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Light_Conditions: string (nullable = true)\n",
      " |-- Local_Authority_(District): string (nullable = true)\n",
      " |-- Carriageway_Hazards: string (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Number_of_Casualties: integer (nullable = true)\n",
      " |-- Number_of_Vehicles: integer (nullable = true)\n",
      " |-- Police_Force: string (nullable = true)\n",
      " |-- Road_Surface_Conditions: string (nullable = true)\n",
      " |-- Road_Type: string (nullable = true)\n",
      " |-- Speed_limit: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Urban_or_Rural_Area: string (nullable = true)\n",
      " |-- Weather_Conditions: string (nullable = true)\n",
      " |-- Vehicle_Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacce1c-cb2b-4fc1-b5c7-632d4c73bd1a",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ üöó Accident Severity and Conditions  \n",
    "\n",
    "## Question  \n",
    "How do **weather conditions, light conditions, and road surface conditions** impact **accident severity**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Group accidents by severity and analyze the impact of weather conditions.  \n",
    "- Identify patterns in different lighting conditions (e.g., daytime vs. nighttime).  \n",
    "- Compare accident severity across different road surface conditions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "605110dc-c99d-4ef2-a42e-46eff70e01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-----------------------+\n",
      "|Accident_Severity|  Weather_Conditions|    Light_Conditions|Road_Surface_Conditions|\n",
      "+-----------------+--------------------+--------------------+-----------------------+\n",
      "|          Serious|  Fine no high winds|            Daylight|                    Dry|\n",
      "|          Serious|  Fine no high winds|            Daylight|            Wet or damp|\n",
      "|           Slight|  Fine no high winds|            Daylight|                    Dry|\n",
      "|          Serious|               Other|            Daylight|           Frost or ice|\n",
      "|          Serious|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|  Fine no high winds|            Daylight|                    Dry|\n",
      "|          Serious|  Fine no high winds|            Daylight|                    Dry|\n",
      "|           Slight|  Fine no high winds|            Daylight|                    Dry|\n",
      "|           Slight|  Fine no high winds|            Daylight|                    Dry|\n",
      "|           Slight|               Other|            Daylight|            Wet or damp|\n",
      "|           Slight|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|Raining no high w...|            Daylight|            Wet or damp|\n",
      "|           Slight|Raining no high w...|            Daylight|            Wet or damp|\n",
      "|           Slight|  Fine no high winds|            Daylight|                    Dry|\n",
      "|          Serious|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "|           Slight|  Fine no high winds|            Daylight|            Wet or damp|\n",
      "|           Slight|  Fine no high winds|Darkness - lights...|                    Dry|\n",
      "+-----------------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected = df_data.select(\n",
    "    \"Accident_Severity\",\n",
    "    \"Weather_Conditions\",\n",
    "    \"Light_Conditions\",\n",
    "    \"Road_Surface_Conditions\"\n",
    ")\n",
    "df_selected.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25dce690-7a05-4435-a4c4-2806af80d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_selected.groupby(\"Weather_Conditions\",\"Accident_Severity\").count().orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd4b469-66c9-497c-aa1f-301de78369ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------+\n",
      "|  Weather_Conditions|Accident_Severity| count|\n",
      "+--------------------+-----------------+------+\n",
      "|  Fine no high winds|           Slight|207574|\n",
      "|  Fine no high winds|          Serious| 33654|\n",
      "|Raining no high w...|           Slight| 30391|\n",
      "|               Other|           Slight| 13318|\n",
      "|Snowing no high w...|           Slight|  4390|\n",
      "|Raining no high w...|          Serious|  4107|\n",
      "|  Fine no high winds|            Fatal|  3230|\n",
      "|Raining + high winds|           Slight|  3036|\n",
      "|   Fine + high winds|           Slight|  2661|\n",
      "|         Fog or mist|           Slight|  1434|\n",
      "|               Other|          Serious|  1403|\n",
      "|Snowing + high winds|           Slight|   476|\n",
      "|Raining + high winds|          Serious|   440|\n",
      "|   Fine + high winds|          Serious|   431|\n",
      "|Snowing no high w...|          Serious|   418|\n",
      "|Raining no high w...|            Fatal|   370|\n",
      "|         Fog or mist|          Serious|   226|\n",
      "|               Other|            Fatal|   137|\n",
      "|Snowing + high winds|          Serious|    61|\n",
      "|   Fine + high winds|            Fatal|    56|\n",
      "+--------------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbe625fc-5f15-4c58-8fb0-e82a6a4bae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_selected.groupby(\"Light_Conditions\",\"Accident_Severity\").count().orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f35f965b-2036-46ab-9ba2-09f24e75ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------+\n",
      "|    Light_Conditions|Accident_Severity| count|\n",
      "+--------------------+-----------------+------+\n",
      "|            Daylight|           Slight|196243|\n",
      "|Darkness - lights...|           Slight| 50716|\n",
      "|            Daylight|          Serious| 28644|\n",
      "|Darkness - no lig...|           Slight| 12801|\n",
      "|Darkness - lights...|          Serious|  8515|\n",
      "|Darkness - no lig...|          Serious|  3078|\n",
      "|Darkness - lighti...|           Slight|  2539|\n",
      "|            Daylight|            Fatal|  2366|\n",
      "|Darkness - lights...|           Slight|   981|\n",
      "|Darkness - lights...|            Fatal|   846|\n",
      "|Darkness - no lig...|            Fatal|   649|\n",
      "|Darkness - lighti...|          Serious|   357|\n",
      "|Darkness - lights...|          Serious|   146|\n",
      "|            Daylight|            fatal|    33|\n",
      "|Darkness - lighti...|            Fatal|    28|\n",
      "|Darkness - lights...|            fatal|    16|\n",
      "|Darkness - lights...|            Fatal|    15|\n",
      "+--------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_weather.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f319b2-91fa-4eed-926a-26cd7830842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_road = df_selected.groupby(\"Road_Surface_Conditions\",\"Accident_Severity\").count().orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a111864-2174-4b27-9df9-336101945989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+------+\n",
      "|Road_Surface_Conditions|Accident_Severity| count|\n",
      "+-----------------------+-----------------+------+\n",
      "|                    Dry|           Slight|177582|\n",
      "|            Wet or damp|           Slight| 70103|\n",
      "|                    Dry|          Serious| 28692|\n",
      "|           Frost or ice|           Slight| 10703|\n",
      "|            Wet or damp|          Serious| 10274|\n",
      "|                   Snow|           Slight|  4287|\n",
      "|                    Dry|            Fatal|  2658|\n",
      "|           Frost or ice|          Serious|  1259|\n",
      "|            Wet or damp|            Fatal|  1088|\n",
      "|                   Snow|          Serious|   443|\n",
      "|   Flood over 3cm. deep|           Slight|   308|\n",
      "|                 Normal|           Slight|   297|\n",
      "|           Frost or ice|            Fatal|   116|\n",
      "|   Flood over 3cm. deep|          Serious|    53|\n",
      "|                    Dry|            fatal|    35|\n",
      "|                   Snow|            Fatal|    28|\n",
      "|                 Normal|          Serious|    19|\n",
      "|            Wet or damp|            fatal|    14|\n",
      "|   Flood over 3cm. deep|            Fatal|    13|\n",
      "|                 Normal|            Fatal|     1|\n",
      "+-----------------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_road.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e232e1d-4528-42e3-8ab5-2db2925f6e6a",
   "metadata": {},
   "source": [
    "# ‚ö° Speed Limit vs. Severity  \n",
    "\n",
    "## Question  \n",
    "- What is the **relationship between speed limits and accident severity**?  \n",
    "- Are higher speed limits linked to **more severe accidents**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Compare accident severity at **different speed limits**.  \n",
    "- Analyze trends in **highway vs. city accidents**.  \n",
    "- Use **regression analysis** to measure correlation between speed and accident severity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ddc828-2b56-438a-8c1a-f1af6cd1db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected2 = df_data.select(\"Speed_limit\",\"Accident_Severity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ed1e2c-7be6-4b18-8491-d40847b8dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Speed_limit|Accident_Severity|\n",
      "+-----------+-----------------+\n",
      "|         30|          Serious|\n",
      "|         30|          Serious|\n",
      "|         30|           Slight|\n",
      "|         30|          Serious|\n",
      "|         30|          Serious|\n",
      "|         30|           Slight|\n",
      "|         30|          Serious|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|          Serious|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "|         30|           Slight|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8b4e2c1-69c4-4f4b-ab4d-256487fa8cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------+\n",
      "|Speed_limit|Accident_Severity| count|\n",
      "+-----------+-----------------+------+\n",
      "|         10|           Slight|     3|\n",
      "|         15|           Slight|     2|\n",
      "|         20|          Serious|   377|\n",
      "|         20|            Fatal|    14|\n",
      "|         20|           Slight|  2508|\n",
      "|         30|            Fatal|  1488|\n",
      "|         30|            fatal|    49|\n",
      "|         30|           Slight|174427|\n",
      "|         30|          Serious| 24076|\n",
      "|         40|           Slight| 21987|\n",
      "|         40|          Serious|  3295|\n",
      "|         40|            Fatal|   368|\n",
      "|         50|           Slight|  8507|\n",
      "|         50|            Fatal|   220|\n",
      "|         50|          Serious|  1464|\n",
      "|         60|          Serious|  8817|\n",
      "|         60|           Slight| 36696|\n",
      "|         60|            Fatal|  1313|\n",
      "|         70|            Fatal|   501|\n",
      "|         70|           Slight| 19150|\n",
      "+-----------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_speed = df_selected2.groupBy(\"Speed_limit\", \"Accident_Severity\").count().orderBy(col(\"Speed_limit\").asc())\n",
    "df_speed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3431039-cfb4-47e7-b796-f16e8401b3f7",
   "metadata": {},
   "source": [
    "# üïí Temporal Accident Patterns  \n",
    "\n",
    "## Question  \n",
    "- What are the **most accident-prone months, days, and time slots**?  \n",
    "- Is there a pattern in **weekdays vs. weekends**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Perform **time-series analysis** to detect accident peaks.  \n",
    "- Compare accident rates between **weekdays and weekends**.  \n",
    "- Visualize accidents across **different times of the day** (morning, afternoon, evening, night).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfeb1504-5f93-4e82-b756-1f146dd9f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+----+\n",
      "|Accident_Index|Accident Date|  Month|Year|Day_of_Week|    Junction_Control|     Junction_Detail|Accident_Severity| Latitude|Light_Conditions|Local_Authority_(District)|Carriageway_Hazards|Longitude|Number_of_Casualties|Number_of_Vehicles|       Police_Force|Road_Surface_Conditions|         Road_Type|Speed_limit|               Time|Urban_or_Rural_Area|Weather_Conditions|        Vehicle_Type|Hour|\n",
      "+--------------+-------------+-------+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+----+\n",
      "| 200901BS70001|   2021-01-01|January|2021|     Friday|Give way or uncon...|T or staggered ju...|          Serious|51.512273|        Daylight|      Kensington and Ch...|               None|-0.201349|                   1|                 2|Metropolitan Police|                    Dry|    One way street|         30|2025-03-20 15:11:00|              Urban|Fine no high winds|                 Car|  15|\n",
      "| 200901BS70002|   2021-01-05|January|2021|    Tuesday|Give way or uncon...|          Crossroads|          Serious|51.514399|        Daylight|      Kensington and Ch...|               None|-0.199248|                  11|                 2|Metropolitan Police|            Wet or damp|Single carriageway|         30|2025-03-20 10:59:00|              Urban|Fine no high winds|Taxi/Private hire...|  10|\n",
      "+--------------+-------------+-------+----+-----------+--------------------+--------------------+-----------------+---------+----------------+--------------------------+-------------------+---------+--------------------+------------------+-------------------+-----------------------+------------------+-----------+-------------------+-------------------+------------------+--------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, date_format, col, month, dayofweek, hour\n",
    "\n",
    "# Convert 'Accident Date' to proper date format\n",
    "df_time = df_data.withColumn(\"Accident Date\", to_date(col(\"Accident Date\"), \"dd-MM-yyyy\")) \\\n",
    "                 .withColumn(\"Month\", date_format(col(\"Accident Date\"), \"MMMM\")) \\\n",
    "                 .withColumn(\"Day_of_Week\", date_format(col(\"Accident Date\"), \"EEEE\")) \\\n",
    "                 .withColumn(\"Hour\", hour(col(\"Time\")))\n",
    "\n",
    "df_time.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cbe053c-5840-4ec5-a443-066bace84778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+\n",
      "|  Month|Day_of_Week|Hour|\n",
      "+-------+-----------+----+\n",
      "|January|     Friday|  15|\n",
      "|January|    Tuesday|  10|\n",
      "+-------+-----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col = df_time.select(\"Month\",\"Day_of_Week\",\"Hour\")\n",
    "df_col.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09fb31-9145-452c-9e5b-e440ac2df049",
   "metadata": {},
   "source": [
    "# üìç Geospatial Accident Hotspots  \n",
    "\n",
    "## Question  \n",
    "Can we identify accident hotspots using **clustering algorithms** (e.g., DBSCAN, K-Means) based on **latitude and longitude**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Use **geospatial clustering** to detect high-risk locations.  \n",
    "- Compare accident hotspots between **urban and rural areas**.  \n",
    "- Use **heatmaps** to visualize accident frequency across different regions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f93fd91-ab2d-46e5-b3de-20fcac3eceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Weekend| count|\n",
      "+-------+------+\n",
      "|Weekday|215878|\n",
      "|Weekend| 92095|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define weekend days based on their names\n",
    "df_weekday_vs_weekend = df_time.withColumn(\n",
    "    \"Weekend\", when(col(\"Day_of_Week\").isin(\"Saturday\", \"Sunday\"), \"Weekend\").otherwise(\"Weekday\")\n",
    ")\n",
    "\n",
    "# Count the number of accidents on weekends vs. weekdays\n",
    "df_weekday_vs_weekend_count = df_weekday_vs_weekend.groupBy(\"Weekend\").count()\n",
    "\n",
    "# Show results\n",
    "df_weekday_vs_weekend_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f82059bb-97a7-4263-a1db-30402098a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|Time_Slot| count|\n",
      "+---------+------+\n",
      "|Afternoon|132398|\n",
      "|    Night| 89697|\n",
      "|  Morning| 85878|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, dayofweek, month, hour,when,desc\n",
    "df_time_slots = df_time.withColumn(\n",
    "    \"Time_Slot\",\n",
    "     when((col(\"Hour\") >= 6) & (col(\"Hour\") <12), \"Morning\")\n",
    "    .when((col(\"Hour\") >= 12) & (col(\"Hour\") <18), \"Afternoon\")\n",
    "    .when((col(\"Hour\") >= 12) & (col(\"Hour\") <18), \"Evening\")\n",
    "    .otherwise(\"Night\")\n",
    "\n",
    ")\n",
    "\n",
    "df_time_slot_count = df_time_slots.groupby(\"Time_Slot\").count().orderBy(desc(\"count\"))\n",
    "df_time_slot_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75fe7a-125f-4a26-aad8-b63fb843d85e",
   "metadata": {},
   "source": [
    "# üöô Vehicle Type and Casualties  \n",
    "\n",
    "## Question  \n",
    "- Which **vehicle types** are involved in the most severe accidents?  \n",
    "- Do certain vehicles correlate with **higher casualty numbers**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Group accidents by **vehicle type** and compare severity levels.  \n",
    "- Identify whether **motorcycles, trucks, or cars** contribute to more casualties.  \n",
    "- Explore casualty trends for **commercial vs. personal vehicles**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ff1708f-8f92-4f84-b0c5-bdfd55a03b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------+\n",
      "|        Vehicle_Type|Accident_Severity| count|\n",
      "+--------------------+-----------------+------+\n",
      "|                 Car|           Slight|205079|\n",
      "|                 Car|          Serious| 31583|\n",
      "|Van / Goods 3.5 t...|           Slight| 13364|\n",
      "|Motorcycle over 5...|           Slight|  9587|\n",
      "|Bus or coach (17 ...|           Slight|  7465|\n",
      "|Motorcycle 125cc ...|           Slight|  5845|\n",
      "|Goods 7.5 tonnes ...|           Slight|  5587|\n",
      "|Taxi/Private hire...|           Slight|  4742|\n",
      "|Motorcycle 50cc a...|           Slight|  3159|\n",
      "|                 Car|            Fatal|  3096|\n",
      "|Motorcycle over 1...|           Slight|  2822|\n",
      "|       Other vehicle|           Slight|  2127|\n",
      "|Van / Goods 3.5 t...|          Serious|  2121|\n",
      "|Goods over 3.5t. ...|           Slight|  2119|\n",
      "|Motorcycle over 5...|          Serious|  1496|\n",
      "|Bus or coach (17 ...|          Serious|  1135|\n",
      "|Motorcycle 125cc ...|          Serious|   926|\n",
      "|Goods 7.5 tonnes ...|          Serious|   878|\n",
      "|Taxi/Private hire...|          Serious|   742|\n",
      "|Minibus (8 - 16 p...|           Slight|   693|\n",
      "+--------------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, desc\n",
    "\n",
    "# Grouping by Vehicle_Type and Accident_Severity,Counts the number of accidents per vehicle type and severity level.\n",
    "df_vehicle_severity = df_data.groupBy(\"Vehicle_Type\", \"Accident_Severity\").count()\n",
    "\n",
    "# Sorting by severity count\n",
    "df_vehicle_severity = df_vehicle_severity.orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "df_vehicle_severity.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d646ef4-abb3-490c-b408-d22fb0b42dc0",
   "metadata": {},
   "source": [
    "# üö¶ Junction Control and Accidents  \n",
    "\n",
    "## Question  \n",
    "Does the **presence or absence of junction control** (e.g., traffic signals, roundabouts) impact accident frequency and severity?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Compare accident rates at **controlled vs. uncontrolled intersections**.  \n",
    "- Identify the **most dangerous types of junctions**.  \n",
    "- Analyze the effect of **traffic signals on accident reduction**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72435a73-2b20-47f5-9daf-ffdad78d5d44",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Compare accident rates at controlled vs. uncontrolled intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f516729-789d-4bed-b447-9a2da8703031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|    Junction_Control| count|\n",
      "+--------------------+------+\n",
      "|Give way or uncon...|150045|\n",
      "|Data missing or o...| 98056|\n",
      "| Auto traffic signal| 32349|\n",
      "|Not at junction o...| 25378|\n",
      "|           Stop sign|  1685|\n",
      "|   Authorised person|   460|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, desc\n",
    "\n",
    "# Count accidents by Junction_Control and sort by count\n",
    "df_junction_control = df_data.groupBy(\"Junction_Control\").count().orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "df_junction_control.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d0a5b-3d78-416a-b3f5-da829940fdbf",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Identify the most dangerous types of junctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bcc3aa3-6129-41a8-8980-c5e365ebc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|     Junction_Detail| count|\n",
      "+--------------------+------+\n",
      "|Not at junction o...|123094|\n",
      "|T or staggered ju...| 96718|\n",
      "|          Crossroads| 29948|\n",
      "|          Roundabout| 27264|\n",
      "|Private drive or ...| 10875|\n",
      "|      Other junction|  8315|\n",
      "|           Slip road|  4265|\n",
      "|More than 4 arms ...|  4148|\n",
      "|     Mini-roundabout|  3346|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_junction_detail = df_data.groupby(\"Junction_Detail\").count().orderBy(desc(\"count\"))\n",
    "df_junction_detail.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb4c4d-3d08-4f24-b7b8-a8d2a3f25d0c",
   "metadata": {},
   "source": [
    "#### 3Ô∏è‚É£ Analyze the effect of traffic signals on accident severity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46a28420-0385-4350-99d6-8aeacd778776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------+\n",
      "|    Junction_Control|Accident_Severity| count|\n",
      "+--------------------+-----------------+------+\n",
      "|Give way or uncon...|           Slight|130585|\n",
      "|Data missing or o...|           Slight| 81128|\n",
      "| Auto traffic signal|           Slight| 28763|\n",
      "|Not at junction o...|           Slight| 20889|\n",
      "|Give way or uncon...|          Serious| 18237|\n",
      "|Data missing or o...|          Serious| 15012|\n",
      "|Not at junction o...|          Serious|  3990|\n",
      "| Auto traffic signal|          Serious|  3284|\n",
      "|Data missing or o...|            Fatal|  1908|\n",
      "|           Stop sign|           Slight|  1499|\n",
      "|Give way or uncon...|            Fatal|  1200|\n",
      "|Not at junction o...|            Fatal|   499|\n",
      "|   Authorised person|           Slight|   416|\n",
      "| Auto traffic signal|            Fatal|   284|\n",
      "|           Stop sign|          Serious|   175|\n",
      "|   Authorised person|          Serious|    42|\n",
      "|Give way or uncon...|            fatal|    23|\n",
      "| Auto traffic signal|            fatal|    18|\n",
      "|           Stop sign|            Fatal|    11|\n",
      "|Data missing or o...|            fatal|     8|\n",
      "+--------------------+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_severity_junction = df_data.groupby(\"Junction_Control\",\"Accident_Severity\").count().orderBy(desc(\"count\"))\n",
    "df_severity_junction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db1111-47f2-4fad-a47e-af5f0a529ea5",
   "metadata": {},
   "source": [
    "# üöóüöï Multi-Vehicle Collisions  \n",
    "\n",
    "## Question  \n",
    "- What percentage of accidents involve **multiple vehicles**?  \n",
    "- How does **severity** change as the number of vehicles increases?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Group accidents by the **number of vehicles involved**.  \n",
    "- Identify whether **multi-vehicle crashes** are more severe.  \n",
    "- Compare **single-vehicle vs. multi-vehicle** accident trends.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b769205a-97c4-4289-8f70-a71717e5876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of accidents involving multiple vehicles: 69.69%\n",
      "+------------------+---------------+--------------------+-------------------+------------------+\n",
      "|Number_of_Vehicles|Total_Accidents|    Fatal_Percentage| Serious_Percentage| Slight_Percentage|\n",
      "+------------------+---------------+--------------------+-------------------+------------------+\n",
      "|                 1|          93349|0.018511178480755015|0.19225701400122122|0.7890496952297293|\n",
      "|                 2|         183595|0.008943598681881314|0.10510090144067104|0.8857866499632343|\n",
      "|                 3|          24226|0.015850738875588213|0.10831338231651944|0.8758358788078924|\n",
      "|                 4|           5077|0.018908804412054364|0.12192239511522553|0.8589718337600946|\n",
      "|                 5|           1127| 0.02839396628216504|0.13043478260869565|0.8411712511091393|\n",
      "|                 6|            346| 0.03468208092485549|0.16184971098265896|0.8034682080924855|\n",
      "|                 7|            127| 0.03937007874015748|0.25984251968503935|0.7007874015748031|\n",
      "|                 8|             63|0.031746031746031744| 0.1746031746031746|0.7936507936507936|\n",
      "|                 9|             31|                 0.0|0.06451612903225806|0.9354838709677419|\n",
      "|                10|             12| 0.16666666666666666|0.16666666666666666|0.6666666666666666|\n",
      "|                11|              6|                 0.0|                0.0|               1.0|\n",
      "|                12|              4|                 0.0|               0.25|              0.75|\n",
      "|                13|              5|                 0.0|                0.2|               0.8|\n",
      "|                14|              2|                 0.0|                0.0|               1.0|\n",
      "|                16|              1|                 1.0|                0.0|               0.0|\n",
      "|                19|              1|                 0.0|                1.0|               0.0|\n",
      "|                32|              1|                 0.0|                0.0|               1.0|\n",
      "+------------------+---------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, avg\n",
    "\n",
    "# Total number of accidents\n",
    "total_accidents = df_data.count()\n",
    "\n",
    "# Count of accidents involving multiple vehicles (more than 1)\n",
    "multi_vehicle_accidents = df_data.filter(col(\"Number_of_Vehicles\") > 1).count()\n",
    "\n",
    "# Percentage of multi-vehicle accidents\n",
    "multi_vehicle_percentage = (multi_vehicle_accidents / total_accidents) * 100\n",
    "print(f\"Percentage of accidents involving multiple vehicles: {multi_vehicle_percentage:.2f}%\")\n",
    "\n",
    "# Group by the number of vehicles and get accident severity distribution\n",
    "df_severity_by_vehicles = (\n",
    "    df_data.groupBy(\"Number_of_Vehicles\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Accidents\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Fatal\", 1).otherwise(0)).alias(\"Fatal_Percentage\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Serious\", 1).otherwise(0)).alias(\"Serious_Percentage\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Slight\", 1).otherwise(0)).alias(\"Slight_Percentage\"),\n",
    "    )\n",
    "    .orderBy(col(\"Number_of_Vehicles\"))\n",
    ")\n",
    "\n",
    "# Show accident severity distribution by number of vehicles\n",
    "df_severity_by_vehicles.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee157a2d-1e9e-440e-bd45-37b674e6268f",
   "metadata": {},
   "source": [
    "# üåÜüèûÔ∏è Rural vs. Urban Accident Trends  \n",
    "\n",
    "## Question  \n",
    "- Are **urban areas more accident-prone** than rural areas?  \n",
    "- How does accident severity differ between these regions?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Compare accident **frequency** in urban vs. rural settings.  \n",
    "- Identify **rural-specific risk factors** (e.g., lack of lighting, sharp turns).  \n",
    "- Use **spatial analysis** to detect accident concentration.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8b3950e-efa9-4a43-9941-c68b74ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+-------------------+------------------+\n",
      "|Urban_or_Rural_Area|Total_Accidents|    Fatal_Percentage| Serious_Percentage| Slight_Percentage|\n",
      "+-------------------+---------------+--------------------+-------------------+------------------+\n",
      "|              Urban|         198532|0.007832490480124112| 0.1184292708480245|0.8734914270747285|\n",
      "|              Rural|         109441|0.021463619667217954|0.15741815224641587|0.8211182280863661|\n",
      "+-------------------+---------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg, when\n",
    "\n",
    "# Count of accidents in Urban vs. Rural areas\n",
    "df_urban_rural = (\n",
    "    df_data.groupBy(\"Urban_or_Rural_Area\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Accidents\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Fatal\", 1).otherwise(0)).alias(\"Fatal_Percentage\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Serious\", 1).otherwise(0)).alias(\"Serious_Percentage\"),\n",
    "        avg(when(col(\"Accident_Severity\") == \"Slight\", 1).otherwise(0)).alias(\"Slight_Percentage\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df_urban_rural.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd631593-5f07-45ad-a85c-e1bd74688280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+----------------+------------------+-----------------+\n",
      "|Urban_or_Rural_Area|Total_Accidents|Fatal_Percentage|Serious_Percentage|Slight_Percentage|\n",
      "+-------------------+---------------+----------------+------------------+-----------------+\n",
      "|              Urban|         198532|            0.78|             11.84|            87.35|\n",
      "|              Rural|         109441|            2.15|             15.74|            82.11|\n",
      "+-------------------+---------------+----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert proportions to percentages\n",
    "df_urban_rural = df_urban_rural.withColumn(\"Fatal_Percentage\", round(col(\"Fatal_Percentage\") * 100, 2))\n",
    "df_urban_rural = df_urban_rural.withColumn(\"Serious_Percentage\", round(col(\"Serious_Percentage\") * 100, 2))\n",
    "df_urban_rural = df_urban_rural.withColumn(\"Slight_Percentage\", round(col(\"Slight_Percentage\") * 100, 2))\n",
    "\n",
    "# Show results\n",
    "df_urban_rural.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217cbdd-2c7d-4e1c-8d4d-1cd6d7dbd236",
   "metadata": {},
   "source": [
    "# üèôÔ∏è District-Level Accident Trends  \n",
    "\n",
    "## Question  \n",
    "- Which **local authority districts** report the highest number of accidents?  \n",
    "- Are certain regions more accident-prone due to **weather, traffic, or road types**?  \n",
    "\n",
    "## Analysis Ideas  \n",
    "- Rank districts by **total accident count**.  \n",
    "- Analyze **district-wise weather impact** on accident severity.  \n",
    "- Identify **high-risk road types** in different districts.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d835205-40c5-4ff0-8bd4-762bf66adf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---------------+\n",
      "|Local_Authority_(District)|Total_Accidents|\n",
      "+--------------------------+---------------+\n",
      "|                Birmingham|           6165|\n",
      "|                     Leeds|           4140|\n",
      "|                Manchester|           3132|\n",
      "|                  Bradford|           3006|\n",
      "|               Westminster|           2811|\n",
      "|                 Sheffield|           2750|\n",
      "|                 Liverpool|           2611|\n",
      "|                  Cornwall|           2606|\n",
      "|                    Barnet|           2302|\n",
      "|          Bristol, City of|           2270|\n",
      "+--------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, desc\n",
    "\n",
    "# Count accidents per district\n",
    "df_district_accidents = df_data.groupBy(\"Local_Authority_(District)\").agg(count(\"*\").alias(\"Total_Accidents\"))\n",
    "\n",
    "# Rank districts by accident count\n",
    "df_district_accidents = df_district_accidents.orderBy(desc(\"Total_Accidents\"))\n",
    "\n",
    "# Show top 10 accident-prone districts\n",
    "df_district_accidents.show(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
